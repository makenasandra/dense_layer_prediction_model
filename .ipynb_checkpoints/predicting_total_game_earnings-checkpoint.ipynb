{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predicting Total Game Earnings using NN\n",
    "In this project we will be using given features to predict total game earnings for each video game \n",
    "<br>The dataset contains video games sold by retailer\n",
    "<br> Each column represents a feature \n",
    "<br>Each row represents a game(training example) \\\n",
    "<br>We look at each characteristic of the game to predict expected earnings\n",
    "<br>We will use Kera for the the front end layer and backend will be powered by Theano\n",
    "<br>Let's begin!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data###\n",
    "We first start by scaling our data using the MinMaxSCaler from the SkLearn library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sandra\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\data.py:323: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: total_earnings values were scaled by multiplying by 0.0000036968 and adding -0.115913\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load training data set from CSV file\n",
    "training_data_df = pd.read_csv(\"sales_data_training.csv\")\n",
    "\n",
    "# Load testing data set from CSV file\n",
    "test_data_df = pd.read_csv(\"sales_data_test.csv\")\n",
    "\n",
    "# Data needs to be scaled to a small range like 0 to 1 for the neural\n",
    "# network to work well.\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Scale both the training inputs and outputs\n",
    "#Fit_transfrom allows us to first fit the scaler to our data... \n",
    "#...that is figure out how much to scale down the values for each feature(in each clomun)...\n",
    "#...then it will scale(transform) the data\n",
    "scaled_training = scaler.fit_transform(training_data_df)\n",
    "#Using transform instead of fit_transfrom allows the scaler to scale down the test data in the same measure as the train data\n",
    "scaled_testing = scaler.transform(test_data_df) \n",
    "\n",
    "# Print out the adjustment that the scaler applied to the total_earnings column of data\n",
    "print(\"Note: total_earnings values were scaled by multiplying by {:.10f} and adding {:.6f}\".format(scaler.scale_[8], scaler.min_[8]))\n",
    "\n",
    "# Create new pandas DataFrame objects from the scaled data(which is are now plain arrays)\n",
    "#This allows us to take advantage of pandas capability to save CSV files which only works for DF objects\n",
    "scaled_training_df = pd.DataFrame(scaled_training, columns=training_data_df.columns.values)\n",
    "scaled_testing_df = pd.DataFrame(scaled_testing, columns=test_data_df.columns.values)\n",
    "\n",
    "# Save scaled data dataframes to new CSV files\n",
    "scaled_training_df.to_csv(\"sales_data_training_scaled.csv\", index=False)\n",
    "scaled_testing_df.to_csv(\"sales_data_testing_scaled.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model\n",
    "We will use the **Keras Sequential Model** which one of the easiest ways to create a model using Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import *\n",
    "\n",
    "#We load the prescaled training data from the newly saved CSV files\n",
    "training_data_df = pd.read_csv(\"sales_data_training_scaled.csv\")\n",
    "\n",
    "#We split the data into two the input values and expected outputs\n",
    "#For X we use all the columns and drop the total_earnings\n",
    "X = training_data_df.drop('total_earnings', axis=1).values\n",
    "\n",
    "#Y only Contains expected earnings for each game\n",
    "Y = training_data_df[['total_earnings']].values\n",
    "\n",
    "# Define the model using Keras Sequential API model\n",
    "#Create Sequential object\n",
    "odel = Sequential()\n",
    "#Create layers\n",
    "#Input layer with 50 densely connected nodes, input size(no.of features) and ReLU activation function\n",
    "model.add(Dense(50, input_dim=9, activation='relu'))\n",
    "\n",
    "#We can add or remove layers as we experiment\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(50, activation='relu'))\n",
    "\n",
    "#Our expected output(total earnings) is a single linear value... \n",
    "#...thus we have one node in dense layer and use linear activation function(which is the default)\n",
    "model.add(Dense(1, activation='linear'))\n",
    "\n",
    "#We use mean_squared_error(mse) loss function to evaluate accuracy of predictions against the actual \n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "**Epoch** - is a single training pass(iteration) across the entire dataset /n\n",
    "<br>**Verobe** - shuffles order of the training data examples randomly as NN train best when data is shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import pandas as pd\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import *\n",
    "\n",
    "# Train the model using fit function\n",
    "model.fit(\n",
    "    X,\n",
    "    Y,\n",
    "    epochs=50,\n",
    "    shuffle=True,\n",
    "    verbose=2\n",
    ")\n",
    "\n",
    "# Load the separate test data set  to determine if the NN has 'learned' \n",
    "test_data_df = pd.read_csv(\"sales_data_test_scaled.csv\")\n",
    "\n",
    "#Separe input and output columns for test data\n",
    "X_test = test_data_df.drop('total_earnings', axis=1).values\n",
    "Y_test = test_data_df[['total_earnings']].values\n",
    "\n",
    "#We use model.evaluate to measure the accuracy of the trained model with the test data set as measured by cost function(mse)\n",
    "test_error_rate = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"The mean squared error (MSE) for the test data set is: {}\".format(test_error_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions with new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data we want to use to make a prediction\n",
    "#The data is already preprocessed\n",
    "X = pd.read_csv(\"proposed_new_product.csv\").values\n",
    "\n",
    "# Make a prediction with the neural network\n",
    "prediction = model.predict(X)\n",
    "\n",
    "#Keras usually assumes that we are going to ask for mutiple predictions with multiple output values... \n",
    "#...thus it always returns predictions as a 2D array\n",
    "# In our case we extract the first element of the first prediction (since that's the only one we have)\n",
    "prediction = prediction[0][0]\n",
    "\n",
    "# Re-scale the data from the 0-to-1 range(scaled form) back to dollars\n",
    "# These constants are from when the data was originally scaled down to the 0-to-1 range\n",
    "prediction = prediction + 0.1159\n",
    "prediction = prediction / 0.0000036968\n",
    "\n",
    "print(\"Earnings Prediction for Proposed Product - ${}\".format(prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the trained model\n",
    "We train the model once and save it to a file so that when we want to use later we can just load it and use it \n",
    "<br>**hf file extension**- is a binary format designed for storing python array data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to disk in hdf5 format\n",
    "#Both the structure and trained weights(that determine how the model works) are saved\n",
    "model.save(\"trained_model.h5\")\n",
    "print(\"Model saved to disk.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load saved model (optional)\n",
    "Once the new file is created and our model is saved in it, we can load the trained neural network from another file using the steps below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "#Loading the model recreated the our entire trained NN\n",
    "model = load_model('trained_model.h5')\n",
    "\n",
    "# Load the data we want to use to make a prediction for\n",
    "X = pd.read_csv(\"proposed_new_product.csv\").values\n",
    "prediction = model.predict(X)\n",
    "\n",
    "# Grab just the first element of the first prediction (since we only have one)\n",
    "prediction = prediction[0][0]\n",
    "\n",
    "# Re-scale the data from the 0-to-1 range back to dollars\n",
    "# These constants are from when the data was originally scaled down to the 0-to-1 range\n",
    "prediction = prediction + 0.1159\n",
    "prediction = prediction / 0.0000036968\n",
    "\n",
    "print(\"Earnings Prediction for Proposed Product - ${}\".format(prediction))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
